{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.1.1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPYX27ivjsNWlaeFYj9mzDR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/secutron/Practice_Ignite/blob/main/2_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLRklxvm-qCv"
      },
      "source": [
        "##2.1.구글 Colab 기반 \n",
        "\n",
        "이그나이트는 작성 시점을 기준으로 다음의 백엔드들을 지원한다.\n",
        "\n",
        "- backends from native torch distributed configuration: “nccl”, “gloo”, “mpi”\n",
        "- XLA on TPUs via pytorch/xla\n",
        "- using Horovod framework as a backend\n",
        "\n",
        "각 의미를 이해하기 위해 가능한 단순한 내용의 코드를 준비하고, 이를 구글 Colab 기반으로 동작시켜 본다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJZgkCZq_n3z"
      },
      "source": [
        "### 2.1.1. CPU단 분산처리\n",
        "\n",
        "  구글 Colab은, 별도로 런타임 유형을 지정하지 않는 경우 GPU나 TPU가 없는 VM(Virtual Machine)이 기본 할당된다. 이는 Colab 페이지의 상단메뉴에서 ‘런타임’ > ‘런타임 유형 변경’ 선택 시 나오는 대화상자가, 아래 그림에서처럼 하드웨어 가속기가 None으로 표시되는 상태인 것으로 확인 가능하다.\n",
        "\n",
        " 이 상태에서 기본 CPU만을 이용하여 분산처리를 진행해보자.  \n",
        "\n",
        "<div align=\"center\">\n",
        "<img width=512 src=\"https://i.imgur.com/WLjhi9E.png\"/>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJjTbUaADfju"
      },
      "source": [
        "####2.1.1.1. 패키지 설치\n",
        "\n",
        " 먼저 현 시점 기준 Colab에서 제공하는 VM은, 이그나이트가 사전 설치되어 있지 않은 상태이다. 따라서 다음과 같이 이그나이트의 최신 version을 설치한다. 참고로 pip 명령어는 package installer for python)의 약자이며, 아래 명령문 실행 시 이그나이트의 pre-release를 PyPI(python package index)로부터 설치하게 된다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejoYfvKluKv8",
        "outputId": "41bacfab-23aa-4c74-dd70-f78736714e0f"
      },
      "source": [
        "!pip install --pre pytorch-ignite"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.5.0.dev20210910-py3-none-any.whl (233 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 71 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 92 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 133 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 143 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 153 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 163 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 174 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 184 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 194 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 204 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 215 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 225 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 233 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.5.0.dev20210910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXOHF3GfEVZo"
      },
      "source": [
        "결과에 따르면, 본 가이드라인 작성 시점의 최신 버전인 xxxx년 x월 xx일자 (예: 2021년 9월 10일자) 이그나이트 x.x.x (예: 0.5.0)가 설치 되었음을 알 수 있다.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZDwXk2mFYAt"
      },
      "source": [
        "#### 2.1.1.2. 노드 1개, 노드 당 프로세스 수 2개 실행\n",
        "\n",
        " 설치가 완료되었다면, 아래의 코드를 실행한다. 코드에서 수행하는 작업은 아래와 같다.\n",
        "\n",
        " (라인 1)에서는 이그나이트의 distributed 패키지를 idist라는 이름으로 불러들이고, (라인 26-28)에서는 백엔드로 gloo를 할당하는 등의 설정 작업을 지정한 후, (라인 22)에서의 training 함수를, (라인 30-31)에서 컨택스트 매니징이 가능한 idist.Parallel을 이용하여 run 시킨다.\n",
        "\n",
        " (라인 22)의 training 함수는 몇 가지 정보를 출력한 후, 무의미한 작업을 반복하도록 작성되었다.\n",
        "\n",
        " 그리고 (라인 27)에서는 분산처리 설정에 해당하는 dist_configs 딕셔너리에 nproc_per_node 키에 해당하는 값으로 2를 설정하였다. 이는 2개의 자식 프로세스를 생성(spawn)하여 분산 처리를 진행하라는 명령으로 생각하면 된다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGYafGpM0CE5",
        "outputId": "4dea6124-df78-4a48-ece0-6d7d435e9b94"
      },
      "source": [
        "import ignite.distributed as idist\n",
        "\n",
        "from functools import wraps\n",
        "import time\n",
        "import random\n",
        "\n",
        "def fn_timer(function):\n",
        "    @wraps(function)\n",
        "    def function_timer(*args, **kwargs):\n",
        "        t0 = time.time()\n",
        "        result = function(*args, **kwargs)\n",
        "        t1 = time.time()\n",
        "        print (idist.get_rank(), \" : Total time running %s: %s seconds\" \n",
        "               % (function.__name__, str(t1-t0)))\n",
        "        return result\n",
        "    return function_timer\n",
        "\n",
        "@fn_timer\n",
        "def random_sort(n):\n",
        "    return sorted([random.random() for i in range(n)])\n",
        "\n",
        "def training(local_rank, config, **kwargs):\n",
        "    print(idist.get_rank(), ': run with config:', config, '- backend=', idist.backend())\n",
        "    random_sort(2500000)\n",
        "\n",
        "backend = 'gloo' # or \"xla-tpu\" or None\n",
        "dist_configs = {'nproc_per_node': 1, \"start_method\": \"fork\"}  # or dist_configs = {...}\n",
        "config = {'c': 12345}\n",
        "\n",
        "with idist.Parallel(backend=backend, **dist_configs) as parallel:\n",
        "    parallel.run(training, config, a=1, b=2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-10 23:43:53,522 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'gloo'\n",
            "2021-09-10 23:43:53,524 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 1\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "\tstart_method: fork\n",
            "2021-09-10 23:43:53,526 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7f3988122950>' in 1 processes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : run with config: {'c': 12345} - backend= gloo\n",
            "0  : Total time running random_sort: 1.3806157112121582 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-10 23:43:55,149 ignite.distributed.launcher.Parallel INFO: End of run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mviaqajTGe6T"
      },
      "source": [
        "결과의 첫번 째 출력 항목은 아래와 같으며, distributed launcher가 gloo 백엔드로 초기화 되었음을 표시한다.\n",
        "  \n",
        "*2021-06-25 07:48:33,646 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'gloo'*\n",
        "<br/>\n",
        "<br/>  \n",
        "\n",
        " 상기 ‘’’1.4 분산 딥러닝 기본 지식’’’ 항목에서 언급된 바와 같이 분산처리를 위해서는 컴퓨팅 코어 간 통신이 필요하며, 본 예제에서는 컴퓨팅 코어가 CPU인 경우이므로 gloo나 mpi이 백엔드로 이용된 것이다. 그리고 만일 이 컴퓨팅 코어가 GPU인 경우 nccl 백엔드 이용이 가능하다. \n",
        "\n",
        " 이와 관련해서는 DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED 페이지의 [rule of thumb](https://pytorch.org/docs/stable/distributed.html) 항목을 참조한다. ~~분산처리의 역사만큼이나 다양한 분산처리 방식이 존재한다. TCP 등을 이용해 직접 프로세스간 통신을 처리하는 방법도 있지만, 하이레벨 관점에서 CPU는 gloo, GPU는 nccl, TPU는 xla 백엔드를 사용해야 한다고 생각하는 것이 정신건강에 좋다.~~\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYjZkajX0XSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d68ecf-5bb4-46d2-c6f6-9d5f9c30e62a"
      },
      "source": [
        "dist_configs['nproc_per_node'] = 2\n",
        "\n",
        "with idist.Parallel(backend=backend, **dist_configs) as parallel:\n",
        "    parallel.run(training, config, a=1, b=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-28 09:34:53,152 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'gloo'\n",
            "2021-06-28 09:34:53,154 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 2\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "\tstart_method: fork\n",
            "2021-06-28 09:34:53,156 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7fb2e079db00>' in 2 processes\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 : run with config: {'c': 12345} - backend= gloo\n",
            "1 : run with config: {'c': 12345} - backend= gloo\n",
            "1  : Total time running random_sort: 2.368380308151245 seconds\n",
            "0  : Total time running random_sort: 2.4123685359954834 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-06-28 09:34:55,941 ignite.distributed.launcher.Parallel INFO: End of run\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UknW9_J69YMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d46ff3-7b0f-4b55-fc6f-9694a2783087"
      },
      "source": [
        "dist_configs['nproc_per_node'] = 8\n",
        "\n",
        "with idist.Parallel(backend=backend, **dist_configs) as parallel:\n",
        "    parallel.run(training, config, a=1, b=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-28 09:34:55,957 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'gloo'\n",
            "2021-06-28 09:34:55,963 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 8\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "\tstart_method: fork\n",
            "2021-06-28 09:34:55,973 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7fb2e079db00>' in 8 processes\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3 : run with config: {'c': 12345} - backend= gloo\n",
            "2 : run with config: {'c': 12345} - backend= gloo\n",
            "5 : run with config: {'c': 12345} - backend= gloo\n",
            "4 : run with config: {'c': 12345} - backend= gloo\n",
            "7 : run with config: {'c': 12345} - backend= gloo\n",
            "1 : run with config: {'c': 12345} - backend= gloo\n",
            "0 : run with config: {'c': 12345} - backend= gloo\n",
            "6 : run with config: {'c': 12345} - backend= gloo\n",
            "1  : Total time running random_sort: 10.030449867248535 seconds\n",
            "7  : Total time running random_sort: 10.167544603347778 seconds\n",
            "0  : Total time running random_sort: 10.17899227142334 seconds\n",
            "4  : Total time running random_sort: 10.21057939529419 seconds\n",
            "3  : Total time running random_sort: 10.241017818450928 seconds\n",
            "5  : Total time running random_sort: 10.342675685882568 seconds\n",
            "2  : Total time running random_sort: 10.453433752059937 seconds\n",
            "6  : Total time running random_sort: 10.57030439376831 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-06-28 09:35:08,860 ignite.distributed.launcher.Parallel INFO: End of run\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ1T5pft0mtM",
        "outputId": "1244fe21-2cfd-47c9-d98b-1707d99bb10a"
      },
      "source": [
        "dist_configs['nproc_per_node'] = 50\n",
        "\n",
        "with idist.Parallel(backend=backend, **dist_configs) as parallel:\n",
        "    parallel.run(training, config, a=1, b=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-28 09:35:08,874 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'gloo'\n",
            "2021-06-28 09:35:08,886 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 50\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "\tstart_method: fork\n",
            "2021-06-28 09:35:08,889 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7fb2e079db00>' in 50 processes\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "42 : run with config: {'c': 12345} - backend= gloo\n",
            "36 : run with config: {'c': 12345} - backend= gloo\n",
            "34 : run with config: {'c': 12345} - backend= gloo\n",
            "37 : run with config: {'c': 12345} - backend= gloo\n",
            "32 : run with config: {'c': 12345} - backend= gloo\n",
            "18 : run with config: {'c': 12345} - backend= gloo\n",
            "15 : run with config: {'c': 12345} - backend= gloo\n",
            "33 : run with config: {'c': 12345} - backend= gloo\n",
            "24 : run with config: {'c': 12345} - backend= gloo\n",
            "31 : run with config: {'c': 12345} - backend= gloo\n",
            "38 : run with config: {'c': 12345} - backend= gloo\n",
            "23 : run with config: {'c': 12345} - backend= gloo\n",
            "41 : run with config: {'c': 12345} - backend= gloo\n",
            "25 : run with config: {'c': 12345} - backend= gloo\n",
            "20 : run with config: {'c': 12345} - backend= gloo\n",
            "44 : run with config: {'c': 12345} - backend= gloo\n",
            "27 : run with config: {'c': 12345} - backend= gloo\n",
            "21 : run with config: {'c': 12345} - backend= gloo\n",
            "22 : run with config: {'c': 12345} - backend= gloo\n",
            "28 : run with config: {'c': 12345} - backend= gloo\n",
            "40 : run with config: {'c': 12345} - backend= gloo\n",
            "30 : run with config: {'c': 12345} - backend= gloo\n",
            "9 : run with config: {'c': 12345} - backend= gloo\n",
            "2 : run with config: {'c': 12345} - backend= gloo\n",
            "26 : run with config: {'c': 12345} - backend= gloo\n",
            "39 : run with config: {'c': 12345} - backend= gloo\n",
            "29 : run with config: {'c': 12345} - backend= gloo\n",
            "48 : run with config: {'c': 12345} - backend= gloo\n",
            "35 : run with config: {'c': 12345} - backend= gloo\n",
            "43 : run with config: {'c': 12345} - backend= gloo\n",
            "14 : run with config: {'c': 12345} - backend= gloo\n",
            "19 : run with config: {'c': 12345} - backend= gloo\n",
            "3 : run with config: {'c': 12345} - backend= gloo\n",
            "46 : run with config: {'c': 12345} - backend= gloo\n",
            "49 : run with config: {'c': 12345} - backend= gloo\n",
            "17 : run with config: {'c': 12345} - backend= gloo\n",
            "12 : run with config: {'c': 12345} - backend= gloo\n",
            "45 : run with config: {'c': 12345} - backend= gloo\n",
            "47 : run with config: {'c': 12345} - backend= gloo\n",
            "0 : run with config: {'c': 12345} - backend= gloo\n",
            "1 : run with config: {'c': 12345} - backend= gloo\n",
            "10 : run with config: {'c': 12345} - backend= gloo\n",
            "13 : run with config: {'c': 12345} - backend= gloo\n",
            "11 : run with config: {'c': 12345} - backend= gloo\n",
            "16 : run with config: {'c': 12345} - backend= gloo\n",
            "5 : run with config: {'c': 12345} - backend= gloo\n",
            "6 : run with config: {'c': 12345} - backend= gloo\n",
            "4 : run with config: {'c': 12345} - backend= gloo\n",
            "8 : run with config: {'c': 12345} - backend= gloo\n",
            "7 : run with config: {'c': 12345} - backend= gloo\n",
            "41  : Total time running random_sort: 64.33436441421509 seconds\n",
            "37  : Total time running random_sort: 64.7222809791565 seconds\n",
            "45  : Total time running random_sort: 64.71990895271301 seconds\n",
            "15  : Total time running random_sort: 64.92481064796448 seconds\n",
            "7  : Total time running random_sort: 64.78663611412048 seconds\n",
            "25  : Total time running random_sort: 65.07219743728638 seconds\n",
            "21  : Total time running random_sort: 65.1572163105011 seconds\n",
            "34  : Total time running random_sort: 65.21848964691162 seconds\n",
            "4  : Total time running random_sort: 65.00434446334839 seconds\n",
            "48  : Total time running random_sort: 65.27094793319702 seconds\n",
            "28  : Total time running random_sort: 65.32453632354736 seconds\n",
            "26  : Total time running random_sort: 65.30035090446472 seconds\n",
            "40  : Total time running random_sort: 65.32968139648438 seconds\n",
            "36  : Total time running random_sort: 65.36366844177246 seconds\n",
            "24  : Total time running random_sort: 65.3502926826477 seconds\n",
            "6  : Total time running random_sort: 65.17905116081238 seconds\n",
            "43  : Total time running random_sort: 65.55514287948608 seconds\n",
            "46  : Total time running random_sort: 65.47486281394958 seconds\n",
            "27  : Total time running random_sort: 65.60959529876709 seconds\n",
            "33  : Total time running random_sort: 65.61063241958618 seconds\n",
            "9  : Total time running random_sort: 65.36535406112671 seconds\n",
            "1  : Total time running random_sort: 65.4566400051117 seconds\n",
            "35  : Total time running random_sort: 65.57419109344482 seconds\n",
            "32  : Total time running random_sort: 65.81053566932678 seconds\n",
            "2  : Total time running random_sort: 65.7938802242279 seconds\n",
            "17  : Total time running random_sort: 65.766033411026 seconds\n",
            "10  : Total time running random_sort: 65.65093636512756 seconds\n",
            "16  : Total time running random_sort: 65.6157615184784 seconds\n",
            "13  : Total time running random_sort: 65.6959981918335 seconds\n",
            "3  : Total time running random_sort: 65.885986328125 seconds\n",
            "38  : Total time running random_sort: 66.00599980354309 seconds\n",
            "0  : Total time running random_sort: 65.77286028862 seconds\n",
            "19  : Total time running random_sort: 65.9303469657898 seconds\n",
            "31  : Total time running random_sort: 66.01006531715393 seconds\n",
            "47  : Total time running random_sort: 65.83582544326782 seconds\n",
            "5  : Total time running random_sort: 65.79355502128601 seconds\n",
            "18  : Total time running random_sort: 66.02154779434204 seconds\n",
            "30  : Total time running random_sort: 65.97219014167786 seconds\n",
            "44  : Total time running random_sort: 66.08739948272705 seconds\n",
            "42  : Total time running random_sort: 66.12819719314575 seconds\n",
            "20  : Total time running random_sort: 66.1553566455841 seconds\n",
            "14  : Total time running random_sort: 66.18195486068726 seconds\n",
            "12  : Total time running random_sort: 66.12871289253235 seconds\n",
            "39  : Total time running random_sort: 66.30748891830444 seconds\n",
            "22  : Total time running random_sort: 66.27281928062439 seconds\n",
            "8  : Total time running random_sort: 66.01991248130798 seconds\n",
            "23  : Total time running random_sort: 66.38294100761414 seconds\n",
            "29  : Total time running random_sort: 66.40745520591736 seconds\n",
            "11  : Total time running random_sort: 66.26861715316772 seconds\n",
            "49  : Total time running random_sort: 66.60967230796814 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-06-28 09:36:24,578 ignite.distributed.launcher.Parallel INFO: End of run\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7d5F7ak0paZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af7c511-cc82-4773-af68-1681719c3d03"
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4599.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4599.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORIklAPc0_Fn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}