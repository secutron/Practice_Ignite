{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.1.2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2fmiw+wErh7PxxTM1PP8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/secutron/Practice_Ignite/blob/main/2_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLRklxvm-qCv"
      },
      "source": [
        "##2.1.구글 Colab 기반 \n",
        "\n",
        "이그나이트는 작성 시점을 기준으로 다음의 백엔드들을 지원한다.\n",
        "\n",
        "- backends from native torch distributed configuration: “nccl”, “gloo”, “mpi”\n",
        "- XLA on TPUs via pytorch/xla\n",
        "- using Horovod framework as a backend\n",
        "\n",
        "각 의미를 이해하기 위해 가능한 단순한 내용의 코드를 준비하고, 이를 구글 Colab 기반으로 동작시켜 본다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJZgkCZq_n3z"
      },
      "source": [
        "### 2.1.2. GPU단 분산처리\n",
        "\n",
        " 구글 Colab은, 별도로 런타임 유형을 지정하지 않는 경우 GPU나 TPU가 없는 VM(Virtual Machine)이 기본 할당된다. GPU단에서 이그나이트를 사용해보기 위해서는, Colab 페이지의 상단메뉴에서 ‘런타임’ > ‘런타임 유형 변경’ 선택 시 나오는 대화상자에서, 하드웨어 가속기를 아래 그림에서처럼 GPU로 변경한다.\n",
        "\n",
        " 만일 가이드라인 문서에서 제시된 링크([2_1_2.ipynb](https://github.com/secutron/Practice_Ignite/blob/main/2_1_2.ipynb))를 통해 본 colab 페이지에 접근한 경우라면, 이미 GPU 사용으로 지정되어 있는 상태일 것이다. ~~마지막으로 사용한 런타임 유형 상태가 저장된다~~\n",
        "  \n",
        "\n",
        "<div align=\"center\">\n",
        "<img width=512 src=\"https://i.imgur.com/9p3BSJn.png\"/>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJjTbUaADfju"
      },
      "source": [
        "####2.1.2.1. 패키지 설치\n",
        "\n",
        " 먼저 현 시점 기준 Colab에서 제공하는 VM은, 이그나이트가 사전 설치되어 있지 않은 상태이다. 따라서 다음과 같이 이그나이트의 최신 version을 설치한다. 참고로 pip 명령어는 package installer for python)의 약자이며, 아래 명령문 실행 시 이그나이트의 pre-release를 PyPI(python package index)로부터 설치하게 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejoYfvKluKv8",
        "outputId": "ade70705-d178-4d27-dfba-fdae8ecc5c2b"
      },
      "source": [
        "!pip install --pre pytorch-ignite"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.5.0.dev20210911-py3-none-any.whl (233 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 233 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.5.0.dev20210911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXOHF3GfEVZo"
      },
      "source": [
        "결과에 따르면, 본 가이드라인 작성 시점의 최신 버전인 xxxx년 x월 xx일자 (예: 2021년 9월 10일자) 이그나이트 x.x.x (예: 0.5.0)가 설치 되었음을 알 수 있다.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZDwXk2mFYAt"
      },
      "source": [
        "#### 2.1.1.2. 노드 1개, 노드 당 프로세스 수 1개 실행\n",
        "\n",
        " 설치가 완료되었다면, 아래의 코드를 실행한다. 코드에서 수행하는 작업은 아래와 같다.\n",
        "\n",
        "~~코드라인 22번부터 31번까지인 idist.Parallel 컨텍스트 매니징 방식만 주의해서 보자. 그 이외는 중요하지 않다~~\n",
        "\n",
        "  (라인 1)에서는 이그나이트의 distributed 패키지를 idist라는 이름으로 불러들이고, (라인 32-34)에서는 백엔드로 nccl를 할당하는 등의 설정 작업을 지정한 후, (라인 27)에서의 training 함수를, (라인 36-37)에서 컨택스트 매니징이 가능한 idist.Parallel을 이용하여 run 시킨다.\n",
        "\n",
        " (라인 27)의 training 함수는 몇 가지 정보를 출력한 후, 무의미한 작업을 반복하도록 작성되었다.\n",
        "\n",
        " 그리고 (라인 27)에서는 분산처리 설정에 해당하는 dist_configs 딕셔너리에 nproc_per_node 키에 해당하는 값으로 1를 설정하였다. Colab에서는 GPU를 1개만 지원하므로, 1개의 자식 프로세스를 생성(spawn)하여 처리를 진행하라는 명령이다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGYafGpM0CE5",
        "outputId": "0965fea1-17a1-4298-85bb-692c52b1b535"
      },
      "source": [
        "import ignite.distributed as idist\n",
        "\n",
        "from functools import wraps\n",
        "import time\n",
        "import random\n",
        "\n",
        "import torch\n",
        "\n",
        "def fn_timer(function):\n",
        "   @wraps(function)\n",
        "   def function_timer(*args, **kwargs):\n",
        "       t0 = time.time()\n",
        "       result = function(*args, **kwargs)\n",
        "       t1 = time.time()\n",
        "       print (idist.get_rank(), \" : Total time running %s: %s seconds\"\n",
        "              % (function.__name__, str(t1-t0)))\n",
        "       return result\n",
        "   return function_timer\n",
        "\n",
        "@fn_timer\n",
        "def torch_random_sort(n):\n",
        "   for _ in range(n):\n",
        "       x = torch.randn(3,4)\n",
        "       x = x.to(idist.device())\n",
        "       sorted, indices = torch.sort(x)\n",
        "\n",
        "def training(local_rank, config, **kwargs):\n",
        "   print(idist.get_rank(), ': run with config:', config, '- backend=', idist.backend())\n",
        "   torch_random_sort(100000) # was 2500000\n",
        "\n",
        "backend = 'nccl' #'gloo' # or \"xla-tpu\" or None\n",
        "dist_configs = {'nproc_per_node': 1, \"start_method\": \"fork\"}  # or dist_configs = {...}\n",
        "config = {'c': 12345}\n",
        "\n",
        "with idist.Parallel(backend=backend, **dist_configs) as parallel:\n",
        "   parallel.run(training, config, a=1, b=2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 02:50:14,653 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'\n",
            "2021-09-11 02:50:14,655 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 1\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "\tstart_method: fork\n",
            "2021-09-11 02:50:14,656 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7f3640329d40>' in 1 processes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : run with config: {'c': 12345} - backend= nccl\n",
            "0  : Total time running torch_random_sort: 12.240988731384277 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 02:50:29,211 ignite.distributed.launcher.Parallel INFO: End of run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mviaqajTGe6T"
      },
      "source": [
        " 결과의 첫번 째 출력 항목은 아래와 같으며, distributed launcher가 nccl 백엔드로 초기화 되었음을 표시한다. \n",
        "~~누군가의 실행에 따라 날짜 등은 계속 변한다. 그리고 이그나이트 버전 변경에 따라 표시되는 내용도 변할 수 있다.~~\n",
        "  \n",
        ">*2021-06-29 01:44:00,550 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'*\n",
        "\n",
        " 상기 ‘’’1.4 분산 딥러닝 기본 지식’’’ 항목에서 언급된 바와 같이 분산처리를 위해서는 컴퓨팅 코어 간 통신이 필요하며, 본 예제에서는 컴퓨팅 코어가 GPU인 경우이므로 nccl이 백엔드로 이용된 것이다. 그리고 만일 이 컴퓨팅 코어가 CPU인 경우 gloo나 mpi 백엔드 이용이 가능하다. \n",
        "\n",
        " 이와 관련해서는 DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED 페이지의 [rule of thumb](https://pytorch.org/docs/stable/distributed.html) 항목을 참조한다. ~~분산처리의 역사만큼이나 다양한 분산처리 방식이 존재한다. TCP 등을 이용해 직접 프로세스간 통신을 처리하는 방법도 있지만, 하이레벨 관점에서 CPU는 gloo, GPU는 nccl, TPU는 xla 백엔드를 사용해야 한다고 생각하는 것이 정신건강에 좋다.~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoFHe3YVOOgZ"
      },
      "source": [
        " 두번 째 출력 항목은 node 1개(nnodes: 1)에서, 노드 당 1개의 처리 작업 (nproc_per_node: 1)이 진행될 것임을 알려준다. \n",
        "\n",
        "\n",
        ">*2021-06-29 01:44:00,551 이그나이트.distributed.launcher.Parallel INFO: - Parameters to spawn processes:*  \n",
        ">>*nproc_per_node: 1*  \n",
        "\t*nnodes: 1*  \n",
        "\t*node_rank: 0*  \n",
        "\t*start_method: fork*  \n",
        "\n",
        " 세번 째 출력 항목은 각 프로세스 내에서 print문에 의한 출력과 측정된 소요 시간을 보여주고 있다. rank가 0번 하나로 표기되었고, 이는 프로세스 총 합이 1개임을 보여 준다.\n",
        "\n",
        ">*2021-06-29 01:44:00,552 이그나이트.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7f475a151320>' in 1 processes*  \n",
        "*0 : run with config: {'c': 12345} - backend= nccl*  \n",
        "*0  : Total time running torch_random_sort: 14.128706693649292 seconds*\n",
        "\n",
        "\n",
        "\n",
        " 그리고 마지막 출력 항목은 분산처리가 모두 끝났음을 알리고 있다.\n",
        "\n",
        ">*2021-06-29 01:44:26,624 이그나이트.distributed.launcher.Parallel INFO: End of run*\n",
        "\n",
        "\n",
        " 여기서 프로세스 시작을 알리는 시간(세번 째 출력 항목)이 01시 44분 00초이고, 분산처리가 모두 끝났음을 알리는 시간(마지막 출력 항목)은 01시 44분 26초임을 확인해 보자. 약 14초 정도의 시간이 소요되는 작업(training 함수)을 총 1번 진행하였으나, 전체 처리 시간은 약 26초이다.\n",
        "\n",
        " 만일 전체 처리 시간에 대해 의문을 가진다면, 기존 코드에서와 달리 GPU로 데이터 전달을 줄인 코드를 추가하고 처리 시간을 다시 측정해본다.\n",
        "  \n",
        " ~~처리 시간은 Colab에서 할당해주는 VM 종류에 따라 변할 수 있다~~\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYjZkajX0XSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab3aaa5-e410-4ccb-b93b-86e17a49f516"
      },
      "source": [
        "@fn_timer\n",
        "def torch_random_sort(n):\n",
        "   x = torch.randn(3,4)\n",
        "   x = x.to(idist.device())\n",
        "   for _ in range(n):\n",
        "       sorted, indices = torch.sort(x)\n",
        "\n",
        "with idist.Parallel(backend=backend, **dist_configs) as parallel:\n",
        "   parallel.run(training, config, a=1, b=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 02:54:58,383 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'\n",
            "2021-09-11 02:54:58,384 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 1\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "\tstart_method: fork\n",
            "2021-09-11 02:54:58,389 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7f3640329d40>' in 1 processes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : run with config: {'c': 12345} - backend= nccl\n",
            "0  : Total time running torch_random_sort: 4.604872703552246 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 02:55:05,330 ignite.distributed.launcher.Parallel INFO: End of run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNNaJKBmy9uq"
      },
      "source": [
        " 출력을 살펴보면, 약 6.7초가 소요되는 작업을 1개의 프로세스에서 처리한 시간이 약 10초로, 전체적인 시간 변동이 발생하고 있다. \n",
        "\n",
        ">*2021-06-29 01:48:00,551 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7f475a151320>' in 1 processes*  \n",
        "*0 : run with config: {'c': 12345} - backend= nccl*  \n",
        "*0  : Total time running torch_random_sort: 6.692128693786271 seconds*\n",
        "*2021-06-29 01:48:10,330 ignite.distributed.launcher.Parallel INFO: End of run*\n",
        "\n",
        " 즉 GPU나 TPU와 같은 가속장치로 데이터를 이동하기 과정에서 오버헤드가 발생하고, 이에 따라 처리시간이 변동할 수 있음을 알아야 한다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKNZb7Kz1h2o"
      },
      "source": [
        "#### 2.1.2.3. 가용 GPU 수량보다 많은 프로세스를 생성\n",
        "\n",
        " Colab으로 할당받은 GPU는 1장 뿐인데, 프로세스를 여러 개 만들어 실행하면 어떻게 될까? 궁금해할 분들을 위해 실행 결과를 보이면 다음과 같다. ~~에러 뿜뿜~~ 그리고 nproc_per_node를 8로 지정해도 결과는 ~에러 뿜품으로~~ 동일하다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "_SFlNsK11z8k",
        "outputId": "496e2f87-0753-4221-9f4b-3739ac08b1bc"
      },
      "source": [
        "dist_configs['nproc_per_node'] = 2\n",
        "\n",
        "with idist.Parallel(backend=backend, **dist_configs) as parallel:\n",
        "    parallel.run(training, config, a=1, b=2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 03:12:17,236 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'\n",
            "2021-09-11 03:12:17,240 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 2\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "\tstart_method: fork\n",
            "2021-09-11 03:12:17,246 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7f3640329d40>' in 2 processes\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ProcessRaisedException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4c7d9dd1ca4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0midist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdist_configs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/distributed/launcher.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;34mf\"Spawn function '{func}' in {self._spawn_params['nproc_per_node']} processes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             )\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0midist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"- Run '{func}' in {idist.get_world_size()} processes\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/distributed/utils.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(backend, fn, args, kwargs_dict, nproc_per_node, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         comp_model_cls.spawn(\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnproc_per_node\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnproc_per_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         )\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/distributed/comp_models/native.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, kwargs_dict, nproc_per_node, nnodes, node_rank, master_addr, master_port, backend, init_method, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                 ),\n\u001b[0;32m--> 399\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mspawn_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             )\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n-- Process %d terminated with the following error:\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moriginal_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mProcessRaisedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n    fn(i, *args)\n  File \"/usr/local/lib/python3.7/dist-packages/ignite/distributed/comp_models/native.py\", line 342, in _dist_worker_task_fn\n    backend, init_method=init_method, world_size=arg_world_size, rank=arg_rank, **kw\n  File \"/usr/local/lib/python3.7/dist-packages/ignite/distributed/comp_models/native.py\", line 73, in create_from_backend\n    backend=backend, init_method=init_method, world_size=world_size, rank=rank, **kwargs\n  File \"/usr/local/lib/python3.7/dist-packages/ignite/distributed/comp_models/native.py\", line 95, in __init__\n    backend, timeout=timeout, init_method=init_method, world_size=world_size, rank=rank, **kwargs\n  File \"/usr/local/lib/python3.7/dist-packages/ignite/distributed/comp_models/native.py\", line 129, in _create_from_backend\n    dist.barrier()\n  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 2524, in barrier\n    work = default_pg.barrier(opts=opts)\nRuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:911, invalid usage, NCCL version 2.7.8\nncclInvalidUsage: This usually reflects invalid usage of NCCL library (such as too many async ops, too many collectives at once, mixing streams in a group, etc).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8fj9UZ51hyZ"
      },
      "source": [
        "여기에서 보이는 주요 오류 메시지는 다음과 같다.\n",
        "\n",
        ">*RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:911, invalid usage, NCCL version 2.7.8\n",
        "ncclInvalidUsage: This usually reflects invalid usage of NCCL library (such as too many async ops, too many collectives at once, mixing streams in a group, etc).*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nutHYlYsZnu2"
      },
      "source": [
        "#### 2.1.2.4. VM 리소스 확인\n",
        "\n",
        " 이제 상기 분산처리 작업을 진행한 Colab의 VM에 대해 확인해보자.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7d5F7ak0paZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b889d13-6284-4c5a-c2b4-9c68f0afbced"
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 85\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "stepping\t: 3\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 1999.999\n",
            "cache size\t: 39424 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 3999.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 85\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "stepping\t: 3\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 1999.999\n",
            "cache size\t: 39424 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 3999.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tju6OyfjZ0l8"
      },
      "source": [
        " 상기 실험 시 Colab으로부터 할당받은 VM은 총 2개의 인텔 제온 프로세서가 탑재되어 있고, 각 프로세서 당 코어 수는 1개이다. GPU를 이용해 작업하였으므로 CPU 정보는 중요하지 않다. 다만 ‘’’2.1.1. CPU 단에서 분산처리'’’ 시 할당받은 VM은 2.30GHz 였는데, 이번에는 2.20GHz를 할당받았다는 차이가 존재한다.  \n",
        "~~즉 VM 할당받을 때마다, 또는 런타임 변경 때마다 달라질 수 있다~~\n",
        "\n",
        "\n",
        ">*processor\t: 0*  \n",
        "...  \n",
        "*model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz*  \n",
        "...  \n",
        "*cpu cores\t: 1*  \n",
        "...  \n",
        "*processor\t: 1*  \n",
        "...  \n",
        "*model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz*  \n",
        "...  \n",
        "*cpu cores\t: 1*  \n",
        "...  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2-4Y5MD2uTT",
        "outputId": "a170b3c8-aaed-4626-972c-8e1a6540caad"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 11 03:16:13 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW4i6A922tjp"
      },
      "source": [
        "할당된 VM은 NVidia Tesla T4를 포함하고 있음을 알 수 있다.\n",
        "\n",
        ">...  \n",
        "*|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |*  \n",
        "*| N/A   37C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |*  \n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoC5obxnaLtj"
      },
      "source": [
        "#### 2.1.1.5. 결과 해석\n",
        "\n",
        " 이제까지 Colab 기반으로 GPU단에서 이그나이트를 이용해 작업을 처리하는 방식을 살펴보았다. 그리고 ‘’’2.1.2.4절의 VM 리소스 확인’’’ 항목에서, 분산처리를 위해 Colab으로부터 할당받은 VM에는 Tesla T4  GPU를 1장 포함하고 있었었음을 확인하였다. \n",
        "\n",
        " 분산처리에서는 여러 개의 프로세스를 생성하여 동시에 작업을 진행시키는 것이 중요하다. 여기서 동시에 작업을 진행시킨다는 의미는 동시에 작업을 하는 것처럼 보이는 것([concurrent computing](https://en.wikipedia.org/wiki/Concurrent_computing))이 아니라, 말 그대로 동시에 작업이 진행되는 것([parallel computing](https://en.wikipedia.org/wiki/Parallel_computing))을 말한다. \n",
        "\n",
        "본 가이드라인에서 보이는 바와 같이 이그나이트 분산처리 프레임워크를 직접적으로 이용하는 경우, GPU 1장에서는 프로세스가 1개만 실행될 수 있다라고 생각해야 한다. 멀티코어(multi-core)인 CPU 에서와는 달리, 매니코어(mani-core)인 GPU에서는 시분할 스케쥴링을 통해 여러 개의 프로세스를 동시에 처리하지 않기 때문이다. ~~다만 분산처리에는 역사만큼이나 다양한 방식이 있어서, GPU에 탑재된 컴퓨팅 코어가 단순하기는 하지만, 이를 시분할로 이용하고자 하는 시도도 있다.~~ 따라서 이그나이트 경우에는, GPU에서 한 시점에 실행되는 프로세스는 코어당 1개가 아니라 보드 당 1개이다. ~~일단 그렇다고 생각하자~~\n",
        "\n",
        " GPU가 여러 장 확보되는 경우, 상기 코드의 nproc_per_node 항목에 적절한 숫자를 할당해 주는 것만으로 코드를 재사용할 수 있다. 이는 이후 GCP 또는 OnPrem 환경을 다루는 항목에서 다시 설명한다. \n",
        "\n",
        " 그리고 아래 그림은 wandb(weight and bias) tool을 이용하여 추적한 GPU utilization 확인 결과이다. \n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "<img width=512 src=\"https://i.imgur.com/hmV4Nmp.png\"/>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg_WWR8npVez"
      },
      "source": [
        "Note: This is not an official [LG AI Research](https://www.lgresearch.ai/) product but sample code provided for an educational purpose\n",
        "\n",
        "<br/>\n",
        "author: John H. Kim\n",
        "<br/>  \n",
        "email: john.kim@lgresearch.ai / secutron@naver.com  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORIklAPc0_Fn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}